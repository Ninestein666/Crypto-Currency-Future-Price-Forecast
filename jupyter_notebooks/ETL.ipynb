{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0aStgWSO0E0E"
      },
      "source": [
        "# **(Crypto Currency Future Price Forecast ETL)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1eLEkw5O0ECa"
      },
      "source": [
        "## Objectives\n",
        "\n",
        "Purpose of this ETL:\n",
        "- prepare a clean, consistent dataset for multi-coin analysis and forecasting (top coins I chose: BTC, DOGE, ETH, HBAR, QNT, SOL, XDC, XLM, XRP).\n",
        "\n",
        "What I will do\n",
        "1) Load each coin CSV from `data/raw/`\n",
        "2) Look at the head/shape/info so I understand what’s inside\n",
        "3) Standardise column names (Date, Open, High, Low, Close, Volume), fix dtypes\n",
        "4) Remove exact duplicates, handle missing values safely\n",
        "5) Create helpful features (daily returns, log returns, 7/30-day moving averages, 30-day volatility)\n",
        "6) Export tidy “long” table for the whole portfolio + per-coin processed CSVs\n",
        "7) Save quick summary reports (coverage, recent volume, correlations) for my README/Power BI\n",
        "\n",
        "\n",
        "## Inputs\n",
        "- Raw Data: DataSet>Raw>BTC.csv\n",
        "- Raw Data: DataSet>Raw>DOGE.csv\n",
        "- Raw Data: DataSet>Raw>ETH.csv\n",
        "- Raw Data: DataSet>Raw>HBAR.csv\n",
        "- Raw Data: DataSet>Raw>QNT.csv\n",
        "- Raw Data: DataSet>Raw>SOL.csv\n",
        "- Raw Data: DataSet>Raw>XDC.csv\n",
        "- Raw Data: DataSet>Raw>XLM.csv\n",
        "- Raw Data: DataSet>Raw>XRP.csv\n",
        "\n",
        "## Outputs\n",
        "- Cleaned Data: DataSet>Cleaned>BTC_Processed.csv\n",
        "- Cleaned Data: DataSet>Cleaned>DOGE_Processed.csv\n",
        "- Cleaned Data: DataSet>Cleaned>ETH_Processed.csv\n",
        "- Cleaned Data: DataSet>Cleaned>HBAR_Processed.csv\n",
        "- Cleaned Data: DataSet>Cleaned>QNT_Processed.csv\n",
        "- Cleaned Data: DataSet>Cleaned>SOL_Processed.csv\n",
        "- Cleaned Data: DataSet>Cleaned>XDC_Processed.csv\n",
        "- Cleaned Data: DataSet>Cleaned>XLM_Processed.csv\n",
        "- Cleaned Data: DataSet>Cleaned>XRP_Processed.csv\n",
        "- Cleaned Data: DataSet>Cleaned>All_Coins_Processed.csv\n",
        "\n",
        "\n",
        "\n",
        "## Additional Comments\n",
        "- This section was assisted by AI (ChatGPT-4) to help write a robust CSV loader function as I was encountering load errors due to inconsistent CSV formats from different sources. I provided the AI with examples of the different CSV formats and it generated a function that could handle these variations. I then reviewed and tested the function to ensure it worked correctly with my data.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9uWZXH9LwoQg"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cqP-UeN-z3i2"
      },
      "source": [
        "# Change working directory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* We are assuming you will store the notebooks in a subfolder, therefore when running the notebook in the editor, you will need to change the working directory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aOGIGS-uz3i2"
      },
      "source": [
        "We need to change the working directory from its current folder to its parent folder\n",
        "* We access the current directory with os.getcwd()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "wZfF_j-Bz3i4",
        "outputId": "66943449-1436-4c3d-85c7-b85f9f78349b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'c:\\\\Users\\\\Nine\\\\OneDrive\\\\Documents\\\\VS Code Projects\\\\Crypto-Currency-Future-Price-Forecast\\\\jupyter_notebooks'"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import os\n",
        "current_dir = os.getcwd()\n",
        "current_dir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9MWW8E7lz3i7"
      },
      "source": [
        "We want to make the parent of the current directory the new current directory\n",
        "* os.path.dirname() gets the parent directory\n",
        "* os.chir() defines the new current directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "TwHsQRWjz3i9",
        "outputId": "86849db3-cd2f-4cc5-ebb8-2d0caafa1a2c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "You set a new current directory\n"
          ]
        }
      ],
      "source": [
        "os.chdir(os.path.dirname(current_dir))\n",
        "print(\"You set a new current directory\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M_xPk_Ijz3i-"
      },
      "source": [
        "Confirm the new current directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "vz3S-_kjz3jA",
        "outputId": "00b79ae4-75d0-4a96-d193-ac9ef9847ea2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'c:\\\\Users\\\\Nine\\\\OneDrive\\\\Documents\\\\VS Code Projects\\\\Crypto-Currency-Future-Price-Forecast'"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "current_dir = os.getcwd()\n",
        "current_dir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-mavJ8DibrcQ"
      },
      "source": [
        "# Section 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZY3l0-AxO93d"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uFQo3ycuO-v6"
      },
      "source": [
        "# Section 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Load the raw data and start ETL process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Used AI to help write this function as load errors were occurring\n",
        "# due to inconsistent CSV formats from different sources.\n",
        "\n",
        "\n",
        "def load_and_normalise_coin(symbol: str) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Robust loader for coin CSVs with headers like: ticker,date,open,high,low,close\n",
        "    If Volume is missing, creates Volume=0 so the rest of the pipeline works.\n",
        "    Returns columns: ['Symbol','Date','Open','High','Low','Close','Volume']\n",
        "    \"\"\"\n",
        "    path = DATA_RAW / f\"{symbol}.csv\"\n",
        "    if not path.exists():\n",
        "        raise FileNotFoundError(f\"[{symbol}] Missing file: {path}\")\n",
        "\n",
        "    df = pd.read_csv(path, encoding_errors=\"ignore\")\n",
        "\n",
        "    # drop unnamed index columns if present\n",
        "    for c in list(df.columns):\n",
        "        if str(c).lower().startswith(\"unnamed\"):\n",
        "            df = df.drop(columns=[c])\n",
        "\n",
        "    # normalise names\n",
        "    cols = {c: str(c).strip().lower().replace(\" \", \"_\") for c in df.columns}\n",
        "    df = df.rename(columns=cols)\n",
        "\n",
        "    # Detect columns\n",
        "    # (Your files: ticker,date,open,high,low,close)\n",
        "    date_col  = next((c for c in [\"date\",\"timestamp\",\"time\",\"datetime\"] if c in df.columns), None)\n",
        "    open_col  = next((c for c in [\"open\",\"open_price\"] if c in df.columns), None)\n",
        "    high_col  = \"high\" if \"high\" in df.columns else None\n",
        "    low_col   = \"low\"  if \"low\"  in df.columns else None\n",
        "    close_col = next((c for c in [\"close\",\"close_price\",\"adj_close\",\"adjusted_close\"] if c in df.columns), None)\n",
        "\n",
        "    if date_col is None or open_col is None or high_col is None or low_col is None or close_col is None:\n",
        "        raise ValueError(f\"[{symbol}] Required OHLC columns not found. Got: {list(df.columns)}\")\n",
        "\n",
        "    # Build output\n",
        "    out = pd.DataFrame()\n",
        "    out[\"Symbol\"] = symbol\n",
        "\n",
        "    # Date parsing (string dates assumed)\n",
        "    out[\"Date\"] = pd.to_datetime(df[date_col], errors=\"coerce\")\n",
        "    out = out.dropna(subset=[\"Date\"])\n",
        "\n",
        "    # Prices to numeric\n",
        "    out[\"Open\"]  = pd.to_numeric(df[open_col],  errors=\"coerce\")\n",
        "    out[\"High\"]  = pd.to_numeric(df[high_col],  errors=\"coerce\")\n",
        "    out[\"Low\"]   = pd.to_numeric(df[low_col],   errors=\"coerce\")\n",
        "    out[\"Close\"] = pd.to_numeric(df[close_col], errors=\"coerce\")\n",
        "\n",
        "    # If volume doesn't exist, create it as 0\n",
        "    if \"volume\" in df.columns:\n",
        "        out[\"Volume\"] = pd.to_numeric(df[\"volume\"], errors=\"coerce\").fillna(0)\n",
        "    else:\n",
        "        out[\"Volume\"] = 0.0\n",
        "\n",
        "    # Sort, de-dup, fill trivial OHLC gaps from Close when present\n",
        "    out = out.sort_values(\"Date\").drop_duplicates(subset=[\"Date\"]).reset_index(drop=True)\n",
        "    for col in [\"Open\",\"High\",\"Low\"]:\n",
        "        mask = out[col].isna() & out[\"Close\"].notna()\n",
        "        out.loc[mask, col] = out.loc[mask, \"Close\"]\n",
        "\n",
        "    # Drop rows where all OHLC are missing after cleanup\n",
        "    keep_mask = ~out[[\"Open\",\"High\",\"Low\",\"Close\"]].isna().all(axis=1)\n",
        "    out = out.loc[keep_mask].copy()\n",
        "\n",
        "    return out[[\"Symbol\",\"Date\",\"Open\",\"High\",\"Low\",\"Close\",\"Volume\"]]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[OK] BTC: 5521 rows\n",
            "[OK] DOGE: 3345 rows\n",
            "[OK] ETH: 3674 rows\n",
            "[OK] HBAR: 2169 rows\n",
            "[OK] QNT: 2396 rows\n",
            "[OK] SOL: 1966 rows\n",
            "[OK] XDC: 1813 rows\n",
            "[OK] XLM: 3145 rows\n",
            "[OK] XRP: 3869 rows\n",
            "\n",
            "Combined shape: (27898, 7)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Symbol</th>\n",
              "      <th>Date</th>\n",
              "      <th>Open</th>\n",
              "      <th>High</th>\n",
              "      <th>Low</th>\n",
              "      <th>Close</th>\n",
              "      <th>Volume</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>NaN</td>\n",
              "      <td>2010-07-17</td>\n",
              "      <td>0.04951</td>\n",
              "      <td>0.04951</td>\n",
              "      <td>0.04951</td>\n",
              "      <td>0.04951</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>NaN</td>\n",
              "      <td>2010-07-18</td>\n",
              "      <td>0.04951</td>\n",
              "      <td>0.08585</td>\n",
              "      <td>0.04951</td>\n",
              "      <td>0.08584</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>NaN</td>\n",
              "      <td>2010-07-19</td>\n",
              "      <td>0.08584</td>\n",
              "      <td>0.09307</td>\n",
              "      <td>0.07723</td>\n",
              "      <td>0.08080</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>NaN</td>\n",
              "      <td>2010-07-20</td>\n",
              "      <td>0.08080</td>\n",
              "      <td>0.08181</td>\n",
              "      <td>0.07426</td>\n",
              "      <td>0.07474</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>NaN</td>\n",
              "      <td>2010-07-21</td>\n",
              "      <td>0.07474</td>\n",
              "      <td>0.07921</td>\n",
              "      <td>0.06634</td>\n",
              "      <td>0.07921</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  Symbol       Date     Open     High      Low    Close  Volume\n",
              "0    NaN 2010-07-17  0.04951  0.04951  0.04951  0.04951     0.0\n",
              "1    NaN 2010-07-18  0.04951  0.08585  0.04951  0.08584     0.0\n",
              "2    NaN 2010-07-19  0.08584  0.09307  0.07723  0.08080     0.0\n",
              "3    NaN 2010-07-20  0.08080  0.08181  0.07426  0.07474     0.0\n",
              "4    NaN 2010-07-21  0.07474  0.07921  0.06634  0.07921     0.0"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# \n",
        "raw_frames, failed = [], []\n",
        "\n",
        "for sym in COINS:\n",
        "    try:\n",
        "        df_sym = load_and_normalise_coin(sym)\n",
        "        raw_frames.append(df_sym)\n",
        "        print(f\"[OK] {sym}: {df_sym.shape[0]} rows\")\n",
        "    except Exception as e:\n",
        "        failed.append((sym, str(e)))\n",
        "        print(f\"[FAIL] {sym}: {e}\")\n",
        "\n",
        "if failed:\n",
        "    print(\"\\n Some symbols failed to load:\")\n",
        "    for s, err in failed:\n",
        "        print(f\" - {s}: {err}\")\n",
        "\n",
        "if len(raw_frames) == 0:\n",
        "    raise RuntimeError(\"No coin files loaded — check the failures printed above.\")\n",
        "\n",
        "raw_all = pd.concat(raw_frames, axis=0, ignore_index=True)\n",
        "print(\"\\nCombined shape:\", raw_all.shape)\n",
        "display(raw_all.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Columns: ['Symbol', 'Date', 'Open', 'High', 'Low', 'Close', 'Volume']\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 27898 entries, 0 to 27897\n",
            "Data columns (total 7 columns):\n",
            " #   Column  Non-Null Count  Dtype         \n",
            "---  ------  --------------  -----         \n",
            " 0   Symbol  0 non-null      object        \n",
            " 1   Date    27898 non-null  datetime64[ns]\n",
            " 2   Open    27898 non-null  float64       \n",
            " 3   High    27898 non-null  float64       \n",
            " 4   Low     27898 non-null  float64       \n",
            " 5   Close   27898 non-null  float64       \n",
            " 6   Volume  27898 non-null  float64       \n",
            "dtypes: datetime64[ns](1), float64(5), object(1)\n",
            "memory usage: 1.5+ MB\n",
            "\n",
            "Missing per column:\n",
            " Symbol    27898\n",
            "Date          0\n",
            "Open          0\n",
            "High          0\n",
            "Low           0\n",
            "Close         0\n",
            "Volume        0\n",
            "dtype: int64\n",
            "\n",
            "Exact duplicates (by Symbol+Date): 22377\n",
            "After dropping duplicates: (5521, 7)\n"
          ]
        }
      ],
      "source": [
        "# Health Check\n",
        "\n",
        "print(\"Columns:\", list(raw_all.columns))\n",
        "raw_all.info()\n",
        "print(\"\\nMissing per column:\\n\", raw_all.isna().sum())\n",
        "\n",
        "dup_rows = raw_all.duplicated(subset=[\"Symbol\",\"Date\"]).sum()\n",
        "print(\"\\nExact duplicates (by Symbol+Date):\", dup_rows)\n",
        "if dup_rows > 0:\n",
        "    raw_all = raw_all.drop_duplicates(subset=[\"Symbol\",\"Date\"]).reset_index(drop=True)\n",
        "    print(\"After dropping duplicates:\", raw_all.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Fill small gaps\n",
        "\n",
        "def fill_small_gaps(group: pd.DataFrame, limit_days:int=2) -> pd.DataFrame:\n",
        "    g = group.sort_values(\"Date\").copy()\n",
        "    g[\"Close_ff\"] = g[\"Close\"].ffill(limit=limit_days)\n",
        "    for col in [\"Open\",\"High\",\"Low\"]:\n",
        "        g[col] = g[col].fillna(g[\"Close_ff\"])\n",
        "    g[\"Close\"] = g[\"Close\"].ffill(limit=limit_days)\n",
        "    # Volume is synthetic (0) for your files; keep it 0 or ffill small gaps if present\n",
        "    g[\"Volume\"] = g[\"Volume\"].fillna(0)\n",
        "    return g.drop(columns=[\"Close_ff\"])\n",
        "\n",
        "filled_all = raw_all.groupby(\"Symbol\", group_keys=False).apply(fill_small_gaps)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Index names: [None]\n",
            "Columns    : ['Symbol', 'Date', 'Open', 'High', 'Low', 'Close', 'Volume']\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Symbol</th>\n",
              "      <th>Date</th>\n",
              "      <th>Open</th>\n",
              "      <th>High</th>\n",
              "      <th>Low</th>\n",
              "      <th>Close</th>\n",
              "      <th>Volume</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "Empty DataFrame\n",
              "Columns: [Symbol, Date, Open, High, Low, Close, Volume]\n",
              "Index: []"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# --- fix the index/column clash for 'Symbol' ---\n",
        "\n",
        "# 1) If 'Symbol' is also an index level, drop that index level without creating a new column\n",
        "if getattr(filled_all.index, \"names\", None) and (\"Symbol\" in filled_all.index.names):\n",
        "    filled_all = filled_all.reset_index(level=\"Symbol\", drop=True)\n",
        "\n",
        "# 2) If we somehow still have multiple 'Symbol' columns, keep the first and drop the rest\n",
        "sym_cols = [i for i, c in enumerate(filled_all.columns) if c == \"Symbol\"]\n",
        "if len(sym_cols) > 1:\n",
        "    keep_idx = sym_cols[0]\n",
        "    keep_mask = [True] * filled_all.shape[1]\n",
        "    for i in sym_cols[1:]:\n",
        "        keep_mask[i] = False\n",
        "    filled_all = filled_all.loc[:, keep_mask]\n",
        "\n",
        "# 3) Final sanity check\n",
        "print(\"Index names:\", filled_all.index.names)\n",
        "print(\"Columns    :\", list(filled_all.columns))\n",
        "assert \"Symbol\" in filled_all.columns and (\"Symbol\" not in (filled_all.index.names or [])), \"Symbol must be a plain column.\"\n",
        "\n",
        "# --- feature engineering (same as before) ---\n",
        "def add_features(group: pd.DataFrame) -> pd.DataFrame:\n",
        "    g = group.sort_values(\"Date\").copy()\n",
        "    g[\"return_1d\"]     = g[\"Close\"].pct_change()\n",
        "    g[\"log_return_1d\"] = np.log(g[\"Close\"]).diff()\n",
        "    g[\"ma_7\"]          = g[\"Close\"].rolling(7,  min_periods=3).mean()\n",
        "    g[\"ma_30\"]         = g[\"Close\"].rolling(30, min_periods=10).mean()\n",
        "    g[\"vol_30d\"]       = g[\"return_1d\"].rolling(30, min_periods=10).std() * np.sqrt(365)\n",
        "    return g\n",
        "\n",
        "proc_all = (\n",
        "    filled_all\n",
        "    .groupby(\"Symbol\", sort=False, group_keys=False)\n",
        "    .apply(add_features)\n",
        "    .reset_index(drop=True)\n",
        ")\n",
        "\n",
        "display(proc_all.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved: DataSet\\Cleaned\\crypto_processed_long.csv\n"
          ]
        }
      ],
      "source": [
        "combined_out = DATA_PROCESSED / \"crypto_processed_long.csv\"\n",
        "proc_all.to_csv(combined_out, index=False)\n",
        "\n",
        "for sym in COINS:\n",
        "    proc_all[proc_all[\"Symbol\"]==sym].to_csv(DATA_PROCESSED / f\"{sym}_processed.csv\", index=False)\n",
        "\n",
        "print(\"Saved:\", combined_out)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Exact duplicates (Symbol+Date): 0\n"
          ]
        }
      ],
      "source": [
        "# Duplicate check (Dataset said it was already cleaned but just in case)\n",
        "\n",
        "dups = proc_all.duplicated(subset=[\"Symbol\",\"Date\"]).sum()\n",
        "print(\"Exact duplicates (Symbol+Date):\", dups)\n",
        "# If >0:\n",
        "if dups:\n",
        "    proc_all = proc_all.drop_duplicates(subset=[\"Symbol\",\"Date\"]).reset_index(drop=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Data Practitioner Jupyter Notebook.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    },
    "orig_nbformat": 2
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
